{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94266e7",
   "metadata": {},
   "source": [
    "# How many tiled MatMul do we need?\n",
    "Given that matmul is our limiting operation, how many tiled matmul do we need to process a Bert layer?\n",
    "\n",
    "Assumptions:\n",
    "- Adding partial sum matrices is free\n",
    "- Adding bias from linear layers is free\n",
    "- Moving data to the input of the tiled matmul is fully overlapped with computation (probably realistic with pingpong buffers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ace0c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MatrixMultiplier:\n",
    "    def __init__(self, tile, cycles_per_tile, clock_freq):\n",
    "        self.tile = tile\n",
    "        self.cycles_per_tile = cycles_per_tile\n",
    "        self.clock_freq = clock_freq\n",
    "        \n",
    "        self.total_tiled_muls = 0\n",
    "        self.total_cycles = 0\n",
    "        \n",
    "    def mul(self, a, b):\n",
    "        M = a.size()[-2]\n",
    "        K = a.size()[-1]\n",
    "        N = b.size()[-1]\n",
    "        assert b.size()[-2] == K, f'a size {a.size()} b size {b.size()}'\n",
    "        \n",
    "        M_blocks = math.ceil(M / self.tile)\n",
    "        K_blocks = math.ceil(K / self.tile)\n",
    "        N_blocks = math.ceil(N / self.tile)\n",
    "        \n",
    "        n_tiled_mul = M_blocks * K_blocks * N_blocks\n",
    "        \n",
    "        self.total_tiled_muls += n_tiled_mul\n",
    "        self.total_cycles += n_tiled_mul*self.cycles_per_tile\n",
    "        \n",
    "        return torch.matmul(a, b)\n",
    "        \n",
    "    def summary(self):\n",
    "        print('-'*10)\n",
    "        print(f'Using {self.tile}x{self.tile} tiled matrix multiplication:')\n",
    "        print(f'{self.total_tiled_muls} tiled matmul in total')\n",
    "        print(f'{self.total_cycles} total cycles')\n",
    "        print(f'{self.total_cycles / self.clock_freq} seconds of latency at {self.clock_freq/1e6} MHz')\n",
    "        print(f'{self.clock_freq / self.total_cycles} FPS layer throughput')\n",
    "        print('-'*10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a70a6f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.attention.self.query.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "773555f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(mm, layer, hidden_states):\n",
    "    '''\n",
    "    hidden_states: <bs, seqlen, dmodel>\n",
    "    '''\n",
    "    bs, seqlen, dmodel = hidden_states.size()\n",
    "    num_heads = layer.attention.self.num_attention_heads\n",
    "    dhead = layer.attention.self.attention_head_size\n",
    "    \n",
    "    # Linear transform to get multiple heads. This is a major MAC slurper.\n",
    "#     query_layer = layer.attention.self.query(hidden_states) # <bs, seqlen, dmodel>\n",
    "    query_layer = mm.mul(hidden_states, layer.attention.self.query.weight.data.T)\n",
    "#     key_layer = layer.attention.self.key(hidden_states)     # <bs, seqlen, dmodel>\n",
    "    key_layer = mm.mul(hidden_states, layer.attention.self.key.weight.data.T)\n",
    "#     value_layer = layer.attention.self.value(hidden_states) # <bs, seqlen, dmodel\n",
    "    value_layer = mm.mul(hidden_states, layer.attention.self.value.weight.data.T)\n",
    "    \n",
    "    # Reshape and transpose for multi-head\n",
    "    new_shape = (bs, seqlen, num_heads, dhead)\n",
    "    \n",
    "    query_layer = query_layer.view(new_shape)\n",
    "    key_layer = key_layer.view(new_shape)\n",
    "    value_layer = value_layer.view(new_shape)\n",
    "    \n",
    "    query_layer = query_layer.permute(0,2,1,3) # <bs, num_head, seqlen, dhead>\n",
    "    key_layer = key_layer.permute(0,2,3,1)     # <bs, num_head, dhead, seqlen>\n",
    "    value_layer = value_layer.permute(0,2,1,3) # <bs, num_head, seqlen, dhead>\n",
    "    \n",
    "    # The attention main course\n",
    "    attention_scores = mm.mul(query_layer, key_layer)\n",
    "    attention_scores /= math.sqrt(dhead)\n",
    "    \n",
    "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_probs = layer.attention.self.dropout(attention_probs)\n",
    "    \n",
    "    attention_out = mm.mul(attention_probs, value_layer)\n",
    "    \n",
    "    attention_out = attention_out.permute(0,2,1,3).contiguous()\n",
    "    attention_out = attention_out.view(bs, seqlen, dmodel)\n",
    "    \n",
    "    # It's time for one more linear transform and layer norm\n",
    "    dense_out = mm.mul(attention_out, layer.attention.output.dense.weight.data.T)\n",
    "    dense_out = layer.attention.output.dropout(dense_out)\n",
    "    # Implements the residual connection\n",
    "    layer_out = layer.attention.output.LayerNorm(dense_out + hidden_states)\n",
    "    \n",
    "    return layer_out\n",
    "\n",
    "def ffn(mm, layer, attention_out):\n",
    "    \n",
    "    # Layer 1\n",
    "    output = mm.mul(attention_out, layer.intermediate.dense.weight.data.T)\n",
    "    output = nn.functional.gelu(output)\n",
    "    \n",
    "    # Layer 2\n",
    "    output = mm.mul(output, layer.output.dense.weight.data.T)\n",
    "    output = layer.output.dropout(output)\n",
    "    output = layer.output.LayerNorm(output + attention_out)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bad4323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "layer = model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "838e2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(mm):\n",
    "    fake_in = torch.zeros(1, 512, 768)\n",
    "    attn_out = attention(mm, layer, fake_in)\n",
    "    ffn_out = ffn(mm, layer, attn_out)\n",
    "    mm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9faa6194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Using 16x16 tiled matrix multiplication:\n",
      "892928 tiled matmul in total\n",
      "18751488 total cycles\n",
      "0.06250496 seconds of latency at 300.0 MHz\n",
      "15.998730340760158 FPS layer throughput\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "mm16 = MatrixMultiplier(tile=16, cycles_per_tile=21, clock_freq=300e6)\n",
    "measure(mm16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1a0d29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Using 32x32 tiled matrix multiplication:\n",
      "111616 tiled matmul in total\n",
      "5245952 total cycles\n",
      "0.017486506666666665 seconds of latency at 300.0 MHz\n",
      "57.18695100527035 FPS layer throughput\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "mm32 = MatrixMultiplier(tile=32, cycles_per_tile=(32+15), clock_freq=300e6)\n",
    "measure(mm32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "daebfc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Using 64x64 tiled matrix multiplication:\n",
      "13952 tiled matmul in total\n",
      "1325440 total cycles\n",
      "0.0044181333333333335 seconds of latency at 300.0 MHz\n",
      "226.33993239980686 FPS layer throughput\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "mm64 = MatrixMultiplier(tile=64, cycles_per_tile=(64+31), clock_freq=300e6)\n",
    "measure(mm64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb3235ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Using 128x128 tiled matrix multiplication:\n",
      "1760 tiled matmul in total\n",
      "336160 total cycles\n",
      "0.0011205333333333333 seconds of latency at 300.0 MHz\n",
      "892.4321751546883 FPS layer throughput\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "mm128 = MatrixMultiplier(tile=128, cycles_per_tile=(128+63), clock_freq=300e6)\n",
    "measure(mm128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-fat",
   "language": "python",
   "name": "trans-fat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
