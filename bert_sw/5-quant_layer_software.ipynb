{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd0b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(layer, hidden_states):\n",
    "    '''\n",
    "    Pass in a encoder layer (which holds pretrained weights) and hidden_states input,\n",
    "    and this function performs the same operations as the layer but in a readable fashion.\n",
    "    \n",
    "    hidden_states: <bs, seqlen, dmodel>\n",
    "    '''\n",
    "    bs, seqlen, dmodel = hidden_states.size()\n",
    "    num_heads = layer.attention.self.num_attention_heads\n",
    "    dhead = layer.attention.self.attention_head_size\n",
    "    \n",
    "    # Linear transform to get multiple heads. This is a major MAC slurper.\n",
    "    # Each of these is calling an nn.Linear layer on hidden_states.\n",
    "#     query_layer = layer.attention.self.query(hidden_states) # <bs, seqlen, dmodel>\n",
    "    query_layer = torch.matmul(hidden_states, layer.attention.self.query.weight.T) + layer.attention.self.query.bias\n",
    "    key_layer = layer.attention.self.key(hidden_states)     # <bs, seqlen, dmodel>\n",
    "    value_layer = layer.attention.self.value(hidden_states) # <bs, seqlen, dmodel>\n",
    "    \n",
    "    # Reshape and transpose for multi-head\n",
    "    new_shape = (bs, seqlen, num_heads, dhead)\n",
    "    \n",
    "    query_layer = query_layer.view(new_shape)\n",
    "    value_layer = value_layer.view(new_shape)\n",
    "    key_layer = key_layer.view(new_shape)\n",
    "    \n",
    "    query_layer = query_layer.permute(0,2,1,3) # <bs, num_head, seqlen, dhead>\n",
    "    value_layer = value_layer.permute(0,2,1,3) # <bs, num_head, seqlen, dhead>\n",
    "    # Key is transposed to match dimensions of Query for matmul\n",
    "    key_layer = key_layer.permute(0,2,3,1)     # <bs, num_head, dhead, seqlen>\n",
    "    \n",
    "    # The attention main course\n",
    "    attention_scores = torch.matmul(query_layer, key_layer)\n",
    "    attention_scores /= math.sqrt(dhead)\n",
    "    \n",
    "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_probs = layer.attention.self.dropout(attention_probs)\n",
    "    \n",
    "    # Weighted sum of Values from softmax attention\n",
    "    attention_out = torch.matmul(attention_probs, value_layer)\n",
    "    \n",
    "    attention_out = attention_out.permute(0,2,1,3).contiguous()\n",
    "    attention_out = attention_out.view(bs, seqlen, dmodel)\n",
    "    \n",
    "    # It's time for one more linear transform and layer norm\n",
    "    dense_out = layer.attention.output.dense(attention_out)\n",
    "    dense_out = layer.attention.output.dropout(dense_out)\n",
    "    \n",
    "    # LayerNorm also mplements the residual connection\n",
    "    layer_out = layer.attention.output.LayerNorm(dense_out + hidden_states)\n",
    "    \n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75352287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(layer, attention_out):\n",
    "    '''\n",
    "    Pass in the feedforward layer and attention output. Returns the same result of a FF forward pass.\n",
    "    '''\n",
    "    # Layer 1\n",
    "    output = layer.intermediate.dense(attention_out)\n",
    "    output = nn.functional.gelu(output)\n",
    "    \n",
    "    # Layer 2\n",
    "    output = layer.output.dense(output)\n",
    "    output = layer.output.dropout(output)\n",
    "    output = layer.output.LayerNorm(output + attention_out)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365f2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-fat",
   "language": "python",
   "name": "trans-fat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
