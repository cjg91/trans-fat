{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34b3bc5",
   "metadata": {},
   "source": [
    "# Statically Quantize Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b353d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cd8a69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import glue_compute_metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e4e773cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_quant_roberta import QuantRobertaForSequenceClassification, RobertaConfig\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.data.metrics import simple_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8ce4361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-MRPC were not used when initializing QuantRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing QuantRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QuantRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MRPC were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "qmodel = QuantRobertaForSequenceClassification.from_pretrained('textattack/roberta-base-MRPC')\n",
    "model = RobertaForSequenceClassification.from_pretrained('textattack/roberta-base-MRPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff68ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/oliver/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"dev\". Should be one of ['train', 'validation', 'test'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmrpc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/load.py:1714\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1712\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1713\u001b[0m )\n\u001b[0;32m-> 1714\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_verifications\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_verifications\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/builder.py:763\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    760\u001b[0m     split \u001b[38;5;241m=\u001b[39m {s: s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits}\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m--> 763\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_verifications\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_verifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    775\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/utils/py_utils.py:250\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Singleton\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m disable_tqdm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    253\u001b[0m     disable_tqdm \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m    254\u001b[0m )\n\u001b[1;32m    255\u001b[0m iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_struct\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m data_struct\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/builder.py:794\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    791\u001b[0m     split \u001b[38;5;241m=\u001b[39m Split(split)\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processing_resources(split)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/builder.py:862\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, split: Union[ReadInstruction, Split] \u001b[38;5;241m=\u001b[39m Split\u001b[38;5;241m.\u001b[39mTRAIN, in_memory: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"Constructs a `Dataset`.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    This is the internal implementation to overwrite called when user calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;124;03m        `Dataset`\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     dataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint\u001b[38;5;241m=\u001b[39mfingerprint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/arrow_reader.py:211\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[1;32m    213\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m corresponds to no data!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/arrow_reader.py:184\u001b[0m, in \u001b[0;36mBaseReader.get_file_instructions\u001b[0;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;124;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     file_instructions \u001b[38;5;241m=\u001b[39m \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filetype_suffix\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     files \u001b[38;5;241m=\u001b[39m file_instructions\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/arrow_reader.py:107\u001b[0m, in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, filetype_suffix)\u001b[0m\n\u001b[1;32m    105\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m ReadInstruction\u001b[38;5;241m.\u001b[39mfrom_spec(instruction)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m absolute_instructions \u001b[38;5;241m=\u001b[39m \u001b[43minstruction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _make_file_instructions_from_absolutes(\n\u001b[1;32m    110\u001b[0m     name\u001b[38;5;241m=\u001b[39mname, name2len\u001b[38;5;241m=\u001b[39mname2len, absolute_instructions\u001b[38;5;241m=\u001b[39mabsolute_instructions, filetype_suffix\u001b[38;5;241m=\u001b[39mfiletype_suffix\n\u001b[1;32m    111\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/arrow_reader.py:618\u001b[0m, in \u001b[0;36mReadInstruction.to_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_rel_to_abs_instr(rel_instr, name2len) \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/arrow_reader.py:618\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_rel_to_abs_instr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_instr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/miniforge3/envs/trans-fat/lib/python3.9/site-packages/datasets/arrow_reader.py:433\u001b[0m, in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    431\u001b[0m split \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39msplitname\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name2len:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown split \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(name2len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    434\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m name2len[split]\n\u001b[1;32m    435\u001b[0m from_ \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39mfrom_\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"dev\". Should be one of ['train', 'validation', 'test']."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('glue', 'mrpc', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "03bf91c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>PCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.</s></s>Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So.</s>\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(dataset[0]['sentence1'], dataset[0]['sentence2'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "69402e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('textattack/roberta-base-MRPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9d14ed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/oliver/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-85dd2375886493b9.arrow\n"
     ]
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "971cb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/oliver/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9254fa17d9f1365e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1725\n",
       "})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "672a12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "581864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b3f5e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5feb8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.eval()\n",
    "    preds = None\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            loss = outputs[0]\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = batch['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, batch['labels'].detach().cpu().numpy(), axis=0)\n",
    "        if i % 10 == 0:\n",
    "    #         print(f\"loss: {loss}\")\n",
    "            pass\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    simple_accuracy(preds, out_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b4ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████████████████████▌                                                                                                                                      | 10/54 [03:03<13:52, 18.92s/it]"
     ]
    }
   ],
   "source": [
    "eval_model(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-fat",
   "language": "python",
   "name": "trans-fat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
