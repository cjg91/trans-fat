{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "377e277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00c3f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ad5ea852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input is the first 512 tokens generated from the proposal for this project.\n",
    "text = 'This project aims to implement a transformer layer on a cluster of FPGAs. In recent years transformers have outperformed traditional convolutional neural networks in many fields, but serial performance is dismal and parallel GPU performance is power-intensive. Specialized architectures have been studied little, especially using FPGA platforms. This research will improve transformer inference performance by offloading computationally intensive sections of the network to reconfigurable accelerators running on a cluster of multiple FPGA devices. This research will result in an acceleration architecture for a single layer of a transformer network along with a performance comparison with CPU and GPU baselines. We propose the investigation of distributed transformer inference across a cluster of multiple field programmable gate arrays (FPGAs). This research will investigate the partitioning of a transformer layer across multiple FPGA devices along with networking between FPGAs in the cluster. Transformers have become a dominant machine learning architecture for many domains such as natural language processing, therefore high speed inference is desirable. However, networks sizes and limited FPGA resources often make inference on a single FPGA slow due to limited parallelism and pipeline depth or impossible due to limited resources. The purpose of this research is to explore methods to overcome these challenges by introducing parallelism through multi-FPGA clusters. Transformers are highly parallel neural network architectures which consist of stacks of encoder and decoder layers. These layers consist of many linear transformations on matrices which are represented by matrix-matrix multiplication. Within an encoder/decoder layer there is an opportunity to parallelize both between concurrent general matrix multiplies (GeMM) and within each GeMM. Attempting to serialize these operations on a CPU leads to high execution time and is a poor utilization of the CPU\\'s general purpose architecture. GPUs can deliver high throughput inference for transformers, though they are power-hungry and do not achieve the low latency required by some applications. Both in the datacenter and at the edge, low-latency and efficient inference is desired. Optimally, there would be an architecture that could scale between these two extremes of computational demand. State-of-the-art transformers can contain upwards of 12 layers and multiply matrices on the order of 1024x1024 elements. In addition, the trend of increasing transformer size does not show signs of slowing. This large use of memory and FLOPs leads to difficulty mapping an entire transformer network to a '\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ddc8b",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "This output hidden state is what we care about. After passing this tokenized input through BERT's embedder and it's 12 encoder layers, this is the result. We want to show that our implementation yields the same `last_hidden_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "13c08252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 768]),\n",
       " tensor([[[-3.7457e-01, -6.9886e-01, -4.4155e-04,  ..., -3.0714e-01,\n",
       "           -3.8659e-01,  4.7352e-01],\n",
       "          [-6.7209e-01, -7.5042e-01, -6.9455e-01,  ...,  1.4919e-01,\n",
       "            1.1460e+00,  1.7025e-01],\n",
       "          [-8.8504e-01, -6.3164e-01, -5.9148e-01,  ...,  2.0482e-01,\n",
       "            1.7474e-01,  2.4267e-01],\n",
       "          ...,\n",
       "          [-2.5009e-01,  4.4047e-02, -2.1806e-01,  ...,  1.0060e-01,\n",
       "            2.7695e-01,  8.8146e-01],\n",
       "          [-7.5948e-01,  7.5724e-02, -3.9088e-01,  ..., -4.3433e-01,\n",
       "            2.8015e-01,  7.4720e-01],\n",
       "          [-3.3422e-01, -5.3717e-02,  5.4829e-01,  ...,  5.3513e-01,\n",
       "           -3.9397e-01, -2.6217e-01]]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "output.last_hidden_state.shape, output.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca8d32",
   "metadata": {},
   "source": [
    "### Step 1: Explicitly call model.embeddings and model.encoder\n",
    "Now, do what BERT does in the forward pass, but explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b478f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = model.embeddings(\n",
    "    input_ids=encoded_input['input_ids'],\n",
    "    position_ids=None,\n",
    "    token_type_ids=encoded_input['token_type_ids'],\n",
    "    inputs_embeds=None,\n",
    "    past_key_values_length=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d28bc3",
   "metadata": {},
   "source": [
    "This is the input data we are now working with. It has gone through the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9bf09f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [-0.6485,  0.6739, -0.0932,  ...,  0.4475,  0.6696,  0.1820],\n",
       "         [ 0.3184,  0.3346, -0.0722,  ...,  0.0517, -0.0069, -0.7439],\n",
       "         ...,\n",
       "         [ 0.7621,  0.5828, -0.0454,  ...,  0.3383,  0.0191, -0.0997],\n",
       "         [ 0.9247,  0.4532,  0.6505,  ..., -0.0661, -0.1281, -0.0595],\n",
       "         [-0.2301, -0.4165,  0.3172,  ..., -0.2481,  0.5677, -1.6841]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf91a31",
   "metadata": {},
   "source": [
    "Now pass it through the encoder layers, all 12 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9700afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_mask = model.get_head_mask(None, model.config.num_hidden_layers)\n",
    "extended_attention_mask = model.get_extended_attention_mask(attention_mask=encoded_input['attention_mask'],\n",
    "                                                            input_shape=encoded_input['input_ids'].size(),\n",
    "                                                            device=encoded_input['input_ids'].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6777577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = model.encoder(\n",
    "    embedding_output,\n",
    "    attention_mask=extended_attention_mask,\n",
    "    head_mask=head_mask,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_values=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bede2a",
   "metadata": {},
   "source": [
    "Notice that we got the same result that we did when calling `model.forward()` at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68686bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.7457e-01, -6.9886e-01, -4.4155e-04,  ..., -3.0714e-01,\n",
       "           -3.8659e-01,  4.7352e-01],\n",
       "          [-6.7209e-01, -7.5042e-01, -6.9455e-01,  ...,  1.4919e-01,\n",
       "            1.1460e+00,  1.7025e-01],\n",
       "          [-8.8504e-01, -6.3164e-01, -5.9148e-01,  ...,  2.0482e-01,\n",
       "            1.7474e-01,  2.4267e-01],\n",
       "          ...,\n",
       "          [-2.5009e-01,  4.4047e-02, -2.1806e-01,  ...,  1.0060e-01,\n",
       "            2.7695e-01,  8.8146e-01],\n",
       "          [-7.5948e-01,  7.5724e-02, -3.9088e-01,  ..., -4.3433e-01,\n",
       "            2.8015e-01,  7.4720e-01],\n",
       "          [-3.3422e-01, -5.3717e-02,  5.4829e-01,  ...,  5.3513e-01,\n",
       "           -3.9397e-01, -2.6217e-01]]], grad_fn=<NativeLayerNormBackward0>),)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e84f16",
   "metadata": {},
   "source": [
    "Can you replicate this `encoder_outputs` result very simply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed512e",
   "metadata": {},
   "source": [
    "### Step 2: Call each encoder layer's `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bd4d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = embedding_output\n",
    "for layer_module in model.encoder.layer:\n",
    "    layer_outputs = layer_module(hidden_states)\n",
    "    hidden_states = layer_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65197372",
   "metadata": {},
   "source": [
    "Yes! Call each layer's `forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a49ee6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.7457e-01, -6.9886e-01, -4.4155e-04,  ..., -3.0714e-01,\n",
       "          -3.8659e-01,  4.7352e-01],\n",
       "         [-6.7209e-01, -7.5042e-01, -6.9455e-01,  ...,  1.4919e-01,\n",
       "           1.1460e+00,  1.7025e-01],\n",
       "         [-8.8504e-01, -6.3164e-01, -5.9148e-01,  ...,  2.0482e-01,\n",
       "           1.7474e-01,  2.4267e-01],\n",
       "         ...,\n",
       "         [-2.5009e-01,  4.4047e-02, -2.1806e-01,  ...,  1.0060e-01,\n",
       "           2.7695e-01,  8.8146e-01],\n",
       "         [-7.5948e-01,  7.5724e-02, -3.9088e-01,  ..., -4.3433e-01,\n",
       "           2.8015e-01,  7.4720e-01],\n",
       "         [-3.3422e-01, -5.3717e-02,  5.4829e-01,  ...,  5.3513e-01,\n",
       "          -3.9397e-01, -2.6217e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c49e5b",
   "metadata": {},
   "source": [
    "How low can we go?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede3bf6",
   "metadata": {},
   "source": [
    "### Step 3: For each layer, call it's submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "20594d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = embedding_output\n",
    "for layer_module in model.encoder.layer:\n",
    "    # MHA + LayerNorm\n",
    "    self_attn_out = layer_module.attention(hidden_states)\n",
    "    attn_out = self_attn_out[0]\n",
    "    # First FF\n",
    "    intermediate_out = layer_module.intermediate(attn_out)\n",
    "    # Second FF + LayerNorm\n",
    "    layer_out = layer_module.output(intermediate_out, attn_out)\n",
    "    hidden_states = layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d5ee6a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 768]),\n",
       " tensor([[[-3.7457e-01, -6.9886e-01, -4.4155e-04,  ..., -3.0714e-01,\n",
       "           -3.8659e-01,  4.7352e-01],\n",
       "          [-6.7209e-01, -7.5042e-01, -6.9455e-01,  ...,  1.4919e-01,\n",
       "            1.1460e+00,  1.7025e-01],\n",
       "          [-8.8504e-01, -6.3164e-01, -5.9148e-01,  ...,  2.0482e-01,\n",
       "            1.7474e-01,  2.4267e-01],\n",
       "          ...,\n",
       "          [-2.5009e-01,  4.4047e-02, -2.1806e-01,  ...,  1.0060e-01,\n",
       "            2.7695e-01,  8.8146e-01],\n",
       "          [-7.5948e-01,  7.5724e-02, -3.9088e-01,  ..., -4.3433e-01,\n",
       "            2.8015e-01,  7.4720e-01],\n",
       "          [-3.3422e-01, -5.3717e-02,  5.4829e-01,  ...,  5.3513e-01,\n",
       "           -3.9397e-01, -2.6217e-01]]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape, hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dd60d",
   "metadata": {},
   "source": [
    "### Step 4: Implement the layer's operations\n",
    "Now we use the model just as a repository of each layer's weights. Implement the linear transform, multi-head attention, feedforward, and layernorm. The cell below shows the parameters in one encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "12c0ad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bd0d14c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [159]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(K, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# <bs, dmodel, seqlen>\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview(K, shape\u001b[38;5;241m=\u001b[39m(bs, dmodel\u001b[38;5;241m/\u001b[39mnum_heads))\n\u001b[0;32m---> 26\u001b[0m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [159]\u001b[0m, in \u001b[0;36mattention\u001b[0;34m(layer, hidden_states)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m### TODO: MHA with the right tensor shapes.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(K, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# <bs, dmodel, seqlen>\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(K, shape\u001b[38;5;241m=\u001b[39m(bs, dmodel\u001b[38;5;241m/\u001b[39mnum_heads))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "def attention(layer, hidden_states):\n",
    "    '''\n",
    "    hidden_states: <bs, seqlen, dmodel>\n",
    "    '''\n",
    "    bs, seqlen, dmodel = hidden_states.size()\n",
    "    num_heads = layer.attention.self.attention_head_size\n",
    "    Wq = layer.attention.self.query\n",
    "    Wk = layer.attention.self.key\n",
    "    Wv = layer.attention.self.value\n",
    "    drop1 = layer.attention.self.dropout\n",
    "    \n",
    "    Wg = layer.attention.output.dense\n",
    "    layernorm = layer.attention.output.LayerNorm\n",
    "    drop2 = layer.attention.output.dropout\n",
    "    \n",
    "    # Linear transform to get multiple heads\n",
    "    Q = Wq(hidden_states) # <bs, seqlen, dmodel>\n",
    "    K = Wk(hidden_states) # <bs, seqlen, dmodel>\n",
    "    V = Wv(hidden_states) # <bs, seqlen, dmodel>\n",
    "    ### TODO: MHA with the right tensor shapes.\n",
    "    K = torch.transpose(K, 1, 2) # <bs, dmodel, seqlen>\n",
    "    K = torch.view(K, shape=(bs, dmodel/num_heads))\n",
    "    \n",
    "    \n",
    "    \n",
    "attention(model.encoder.layer[0], embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = embedding_output\n",
    "for layer_module in model.encoder.layer:\n",
    "    # MHA + LayerNorm\n",
    "    self_attn_out = layer_module.attention(hidden_states)\n",
    "    attn_out = self_attn_out[0]\n",
    "    # First FF\n",
    "    intermediate_out = layer_module.intermediate(attn_out)\n",
    "    # Second FF + LayerNorm\n",
    "    layer_out = layer_module.output(intermediate_out, attn_out)\n",
    "    hidden_states = layer_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-fat",
   "language": "python",
   "name": "trans-fat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
